{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Harika Pamulapati\n",
        "\n",
        "Week 09 - Machine Learning with Scikit-learn"
      ],
      "metadata": {
        "id": "slelEC8rj_g3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Among the different classification models included in the Python notebook, which model had the best overall performance? Support your response by referencing appropriate evidence.\n",
        "\n",
        "\n",
        "The standard Logistic Regression model together with Logistic Regression with L1 penalty (C=10) demonstrated optimal classification results among all evaluated models according to the Python notebook results. The models delivered equivalent test accuracy results of 0.718 which stood as the highest performance among all examined models. The Random Forest without cross-validation achieved 0.9993 training accuracy but exhibited 0.686 test accuracy because it overfitted the training data strongly.\n",
        "\n",
        "The Logistic Regression with L1 penalty (C=1) achieved a test accuracy of 0.716 while the Logistic Regression with Standard Scaling and L1 penalty model obtained 0.714. The models with smaller C values and auto-selected C parameters demonstrated inferior performance because this dataset benefited from reduced regularization.\n",
        "\n",
        "The classification models demonstrated superior performance than the null baseline model which achieved 0.608 test accuracy by predicting the dominant class. The improved results indicate that the dataset features hold relevant information which helps classify patients based on their mortality risk.\n",
        "\n",
        "The evaluation of different models should include both predictive accuracy measurements and the ease of interpretation. The Logistic Regression models offer clear coefficient interpretation to identify important features yet Random Forest models tend to show superior performance across other datasets but sacrifice interpretability. The simple Logistic Regression models demonstrated the most optimal combination of accuracy and generalization performance."
      ],
      "metadata": {
        "id": "PjrxFv-rkDxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Next, fit a series of logistic regression models, without regularization. Each model should use the same set of predictors (all of the relevant predictors in the dataset) and should use the entire dataset, rather than a fraction of it. Use a randomly chosen 80% proportion of observations for training and the remaining for checking the generalizable performance (i.e., performance on the holdout subset). Be sure to ensure that the training and holdout subsets are identical across all models. Each model should choose a different solver.\n",
        "\n",
        "3. Compare the results of the models in terms of their accuracy (use this as the performance metric to assess generalizability error on the holdout subset) and the time taken (use appropriate timing function). Summarize your results via a table with the following structure:"
      ],
      "metadata": {
        "id": "IrbFSH9CkGdy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ny8OcuFEjeaY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from patsy import dmatrices\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_patient = pd.read_csv('PatientAnalyticFile.csv')\n",
        "\n",
        "df_patient['mortality'] = np.where(df_patient['DateOfDeath'].isnull(), 0, 1)\n",
        "\n",
        "df_patient['DateOfBirth'] = pd.to_datetime(df_patient['DateOfBirth'])\n",
        "df_patient['Age_years'] = ((pd.to_datetime('2015-01-01') - df_patient['DateOfBirth']).dt.days/365.25)\n",
        "\n",
        "vars_remove = ['PatientID', 'First_Appointment_Date', 'DateOfBirth',\n",
        "               'Last_Appointment_Date', 'DateOfDeath', 'mortality']\n",
        "vars_left = set(df_patient.columns) - set(vars_remove)\n",
        "formula = \"mortality ~ \" + \" + \".join(vars_left)"
      ],
      "metadata": {
        "id": "yenC5cFMkrKU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y, X = dmatrices(formula, df_patient)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, np.ravel(Y), test_size=0.2, random_state=42)\n",
        "\n",
        "solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
        "\n",
        "results = {\n",
        "    'Solver': [],\n",
        "    'Training Accuracy': [],\n",
        "    'Holdout Accuracy': [],\n",
        "    'Time Taken (seconds)': []\n",
        "}"
      ],
      "metadata": {
        "id": "TaF9ATXAku3y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for solver in solvers:\n",
        "    # Time the model fitting\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Create and fit the model - handle different solvers appropriately\n",
        "    if solver == 'liblinear':\n",
        "        model = LogisticRegression(solver=solver, penalty='l2', C=1e6, random_state=42, max_iter=1000)\n",
        "    else:\n",
        "        model = LogisticRegression(solver=solver, penalty= None, random_state=42, max_iter=1000)\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate time taken\n",
        "    time_taken = time.time() - start_time\n",
        "\n",
        "    # Calculate accuracies\n",
        "    train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
        "    test_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "\n",
        "    # Store results\n",
        "    results['Solver'].append(solver)\n",
        "    results['Training Accuracy'].append(round(train_accuracy, 4))\n",
        "    results['Holdout Accuracy'].append(round(test_accuracy, 4))\n",
        "    results['Time Taken (seconds)'].append(round(time_taken, 4))\n",
        "\n",
        "# Create and display results dataframe\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS3_ZPymkzFn",
        "outputId": "b85065d1-f087-47ff-e832-42ee4707c090"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Solver  Training Accuracy  Holdout Accuracy  Time Taken (seconds)\n",
            "0  newton-cg             0.7481            0.7355                0.2995\n",
            "1      lbfgs             0.7481            0.7360                0.5745\n",
            "2  newton-cg             0.7481            0.7355                0.0813\n",
            "3      lbfgs             0.7481            0.7360                0.2418\n",
            "4  newton-cg             0.7481            0.7355                0.0835\n",
            "5      lbfgs             0.7481            0.7360                0.2438\n",
            "6  liblinear             0.7479            0.7362                0.0633\n",
            "7        sag             0.7479            0.7358                2.4845\n",
            "8       saga             0.7480            0.7360                4.0497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Based on the results, which solver yielded the best results? Explain the basis for ranking the models - did you use training subset accuracy? Holdout subset accuracy? Time of execution? All three? Some combination of the three?\n",
        "\n",
        "\n",
        "\n",
        "The liblinear solver stands as the most effective solution for achieving high accuracy at a reasonable execution time. The liblinear solver delivered the maximum holdout accuracy score of 0.7362 and operated at the fastest speed of 0.0633 seconds. The combination of high speed and accuracy performance makes liblinear the most efficient solver for processing this dataset.\n",
        "\n",
        "The lbfgs solver achieved the same holdout accuracy (0.7360) as liblinear but liblinear showed a slight advantage with 0.7362. The lbfgs solver executed at a slower pace than liblinear since it required 0.2438-0.5745 seconds to run while liblinear executed in 0.0633 seconds thus making it less efficient despite achieving similar accuracy levels.\n",
        "\n",
        "The newton-cg solver delivered consistent results with a holdout accuracy of 0.7355 yet failed to surpass liblinear in terms of accuracy or execution speed. The execution times of 2.4845 and 4.0497 seconds for the sag and saga solvers demonstrated their status as the slowest methods while failing to deliver any accuracy benefits to offset this excessive computational cost.\n",
        "\n",
        "Practical machine learning model evaluations should take into account how efficiently models predict and process data. The combination of factors that the liblinear solver provides makes it the ideal selection for running this classification operation."
      ],
      "metadata": {
        "id": "_vreEv6WkI4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X0neJPLVlKLz"
      }
    }
  ]
}